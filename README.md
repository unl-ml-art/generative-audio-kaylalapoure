# Project 2 Generated Confidence

Kayla LaPoure, klapoure2@huskers.unl.edu

## Abstract
I want to reflect on who and what makes me a person, an individual and how to be confident in that. I think many people ponder, especially in the age of digital twins and fitting in: Who am I? Is it me physically? My voice? What if I am not confident in myself? Most of us are told to fake it till you make it, especially in the realm of self-confidence. I would like to explore what “fake” is and how it can be used. To some, “fake” is self affirmation in the mirror, but I want to go farther with it. I want to playback myself being confident, what does it sound and look like on me?

For this project I will be using AI generation to “fake” a confident me. I used a TTS model in Open on Demand and trained it on a clip of me saying positive affirmations. I have combined a video of myself, an AI generated video of myself from a picture, my own voice, and an AI generated voice based off of that. Both voices will read back positive affirmations. The goal isn’t necessarily to be prideful of my own looks or voice, but to poke at what makes me an individual, a human, a digital twin, and confident.  

## Model/Data

I used a text to speech (TTC) model to synthesize/ recreate my own voice. 
I also used the TokkingHeads, which is similar to MyHeritage, animation for the visual component of my projects.
For the training data I recorded my own voice reading different affirmations in hopes to make it easier on the AI to recreate what I was saying.


## Code

Your code for generating your project:
- Jupyter notebooks: https://crane-ood.unl.edu/node/c2422.crane.hcc.unl.edu/26847/doc/tree/ml-art-code/voice_cloning/YourTTS_zeroshot_TTS_demo.ipynbhttps://crane-ood.unl.edu/node/c2422.crane.hcc.unl.edu/26847/doc/tree/ml-art-code/voice_cloning/YourTTS_zeroshot_TTS_demo.ipynb
- There was also an option for CoLab, but there were issues with getting the correct issues of torch to upload.

## Results

I think that the experience of having generated words with a video of myself creates a very weird experience. I see my face moving, but not the way I move it and a voice saying things that I said, just slightly off. Overall, exploring who I am as a person and what makes me up comparatively to AI or my digital twin was really interesting. While there is a big picture that works in both areas, it really is the little things, the subtleties in my voice inflections, my micromovements of my face that make me up. Perhaps that too is what builds up confidence; one syllable at a time. 

Here is the Audio version of my own voice:
 
Here is the audio version of the AI using my voice as the training data:

https://user-images.githubusercontent.com/78117204/162637209-337dae68-d1ef-499e-a9e3-c2d5a9f9fd02.mp4


 
Here’s the video and audio components:

Documentation of your results in an appropriate format, both links to files and a brief description of their contents:
- `.wav` files or `.mp4`
- `.midi` files
- musical scores
- ... some other form

## Technical Notes

Any implementation details or notes we need to repeat your work. 
- Does this code require other pip packages, software, etc?
- Does it run on some other (non-datahub) platform? (CoLab, etc.)

## Reference

References to any papers, techniques, repositories you used:
- Papers
- Repositories
- Blog posts
