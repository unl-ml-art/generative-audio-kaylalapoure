# Project 2 Generated Confidence

Kayla LaPoure, klapoure2@huskers.unl.edu

## Abstract
I want to reflect on who and what makes me a person, an individual and how to be confident in that. I think many people ponder, especially in the age of digital twins and fitting in: Who am I? Is it me physically? My voice? What if I am not confident in myself? Most of us are told to fake it till you make it, especially in the realm of self-confidence. I would like to explore what “fake” is and how it can be used. To some, “fake” is self affirmation in the mirror, but I want to go farther with it. I want to playback myself being confident, what does it sound and look like on me?

For this project I will be using AI generation to “fake” a confident me. I used a TTS model in Open on Demand and trained it on a clip of me saying positive affirmations. I have combined a video of myself, an AI generated video of myself from a picture, my own voice, and an AI generated voice based off of that. Both voices will read back positive affirmations. The goal isn’t necessarily to be prideful of my own looks or voice, but to poke at what makes me an individual, a human, a digital twin, and confident.  

## Model/Data

I used a text to speech (TTC) model to synthesize/ recreate my own voice. 
I also used the TokkingHeads, which is similar to MyHeritage, animation for the visual component of my projects.
For the training data I recorded my own voice reading different affirmations in hopes to make it easier on the AI to recreate what I was saying.


## Code

Your code for generating your project:
- Jupyter notebooks: The notebook is included in the project folder itself instead of linked. 
- There was also an option for CoLab, but there were issues with getting the correct issues of torch to upload. The google drive link is also so old it is broken, so you will need to download it and import it. I included a link to that version below. 
- CoLab notebook: https://colab.research.google.com/drive/1haQYzEAMnOOc9oGLV6bxszWE8CoXOWOC#scrollTo=33efzXGOWDx3
- File Directory to download: https://drive.google.com/uc?id=1sgEjHt0lbPSEw9-FSbC_mBoOPwNi87YR

## Results

I think that the experience of having generated words with a video of myself creates a very weird experience. I see my face moving, but not the way I move it and a voice saying things that I said, just slightly off. Overall, exploring who I am as a person and what makes me up comparatively to AI or my digital twin was really interesting. While there is a big picture that works in both areas, it really is the little things, the subtleties in my voice inflections, my micromovements of my face that make me up. Perhaps that too is what builds up confidence; one syllable at a time. 

Here is the Audio version of my own voice:
I should mention that I was ill when I recorded my voice, so my own voice doesn't quite sound like this.

https://user-images.githubusercontent.com/78117204/162637595-72622f21-5485-496a-9b95-0930e9aed776.mp4

 
Here is the audio version of the AI using my voice as the training data:


https://user-images.githubusercontent.com/78117204/162637589-546fa629-cf7e-457f-b540-120aab540ec8.mp4

 
Here’s the video and audio components:
I had my voice with my own face and the AI face for part of the video and I had the AI voice with my own face and AI face. 

https://youtu.be/ZarxfYlfDJM

## Technical Notes

I used my own voice, so using that recording would help recreate it 
Visually, I chose a photo that is about ¾ of the way facing forward to get the visual results that I wanted from the AI
I used OOD for the sound elements of my project
I used TokkingHeads for the visual elements of the project


## Reference

Affirmations I used: Top 100 List of Positive Affirmations | Committed To Myself

CoLab: YourTTS-zeroshot-TTS-demo.ipynb - Colaboratory (google.com)

TokkingHeads: Tokkingheads
